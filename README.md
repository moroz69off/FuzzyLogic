# FuzzyLogic
>Предметом нечёткой логики считается исследование рассуждений в условиях нечёткости, размытости, сходных с рассуждениями в обычном смысле, и их применение в вычислительных системах.

[Демо :arrow_forward:](https://moroz69off.github.io/FuzzyLogic/)

## Практический пример системы на основе нечеткой логики

Давайте разработаем простую систему нечеткого управления для управления работой стиральной машины, так чтобы нечеткая система контролировала процесс стирки, водозабор, время стирки и скорость отжима.
Входными параметрами здесь являются объем одежды, степень загрязнения и тип грязи. В то время как объем одежды определял бы водозабор, степень загрязнения в свою очередь определялась бы прозрачностью воды, а тип грязи определялся временем, когда цвет воды остается неизменным.

### Первым шагом будет определение лингвистических переменных и терминов.

*Для входных данных лингвистические переменные приведены ниже:*

* Тип грязи: {Greasy, Medium, Not Greasy} (жирное, среднее, не жирное)
* Качество грязи: {Large, Medium, Small} (высокое, среднее, незначительное)

*Для вывода лингвистические переменные приведены ниже:*

* Время стирки: {Short, Very Short, Long, Medium, Very Long} (короткий, очень короткий, длинный, средний, очень длинный).

### Второй шаг включает в себя построение функций принадлежности.

Ниже приведены графики, определяющие функции принадлежности для двух входов.

*Функции принадлежности для качества грязи:*

![Image greasy](https://github.com/moroz69off/FuzzyLogic/blob/master/files/fuzzy5.jpg?raw=true)

*Функции принадлежности для типа грязи:*

![Image mud](https://github.com/moroz69off/FuzzyLogic/blob/master/files/fuzzy4.jpg?raw=true)

### Третий шаг включает разработку набора правил для базы знаний.

Ниже приведен набор правил с использованием логики IF-THEN (если-тогда):

* IF качество грязи Small И Тип грязи Greasy, THEN Время стирки Long.
* IF качество грязи Medium И Тип грязи Greasy, THEN Время стирки Long.
* IF качество грязи Large и тип грязи Greasy, THEN Время стирки Very Long.
* IF качество грязи Small И Тип грязи Medium, THEN Время стирки Medium.
* IF качество грязи Medium И Тип грязи Medium, THEN Время стирки Medium.
* IF качество грязи Large и тип грязи Medium, THEN Время стирки Medium.
* IF качество грязи Small и тип грязи Non-Greasy, THEN Время стирки Very Short.
* IF качество грязи Medium И Тип грязи Non-Greasy, THEN Время стирки Medium.
* IF качество грязи Large и тип грязи Greasy, THEN Время стирки Short.

### Четвёртый шаг. Правила Хебба.

В интернете, в разных учебниках и книгах по теории нейронных сетей можно встретить самые различные формулировки правила Хебба.
Например, Википедия дает нам два правила Хебба:
* **Первое правило Хебба** — Если сигнал перцептрона неверен и равен нулю, то необходимо увеличить веса тех входов, на которые была подана единица.
* **Второе правило Хебба** — Если сигнал перцептрона неверен и равен единице, то необходимо уменьшить веса тех входов, на которые была подана единица.

Правило Хебба основано на правиле, согласно которому 
весовой вектор увеличивается пропорционально входному и обучающему (или выходному) сигналам.
Веса увеличиваются путем добавления произведения ввода и вывода к старому весу:

**W (новый) = w (старый) + x * y**

Итак, у меня в упрощённой модели машины есть три узла: два на входе (степень жирности грязи, сила или величина загрязнения) и один на выходе (время стирки). Назову эти узлы нейронами для удобства дальнейшей работы.
Эти нейроны обрабатывают полученные данные, чтобы выдать желаемый результат. Узлы или нейроны связаны входами, весами соединений и функциями активации.
Основной характеристикой нейронной сети является ее способность к обучению. Нейронные сети обучаются на известных примерах. Как только сеть обучается, ее можно использовать для решения неизвестных значений проблемы.

Мы хотим заставить хеббовскую сеть различать степень и величину загрязнений белья (по данным датчиков прозрачности воды в начале стирки, и спустя некоторое время). Мне потребуются Хеббовские связи между двумя входными узлами и одним выходным. Это и есть простейшая модель "плоской" искуственной нейронной сети. Начальные веса связей сети инициализируются случайными величинами от 0 до 1, ведь в самом начале машина ещё не может "знать" насколько сильно загрязнение, и какова жирность грязи.
После инициализации связей можно приступать к обучению сети: подаем на вход узлов данные с датчиков (в моей модели это ползунки *range* для нелинейного расчёта или выпадающие списки с выбором *select* для линейного), и соответствующим образом модифицируем связи. Это приводит к проблеме выбора обучающих правил. Если мы просто используем идею об усилении связей (не используя при этом уменьшение связей), то мы должны использовать формулу:

**dw = n * ai * aj**, где:

**dw** – изменение веса связи между i входным элементом и выходным элементом **j**,
**ai** – уровень сигнала на входном элементе **i**,
**aj** – уровень сигнала на выходном элементе **j**,
**n** – постоянный множитель, предохраняющий от слишком значительных изменений весов связей.

Понятно, что при значениях входных и выходных данных, больших 0, весовые коэффициенты не могут уменьшаться, - они принимают все большие и большие значения. Здесь правило Хебба необходимо расширить (улучшить). Если вышеприведенную формулу привести к виду:

**dw = n(2ai - 1) aj**,

то веса будут уменьшаться при активностях входных элементов, меньших 0.5 ( 2ai-1 будет давать отрицательное число). Эта ситуация называется Post-Not-Pre LTD (*пост-непре-синаптическое подавление длительной связи*) в том смысле что вес связи будет уменьшаться при неактивном пресинаптическом нейроне и активном постсинаптическом.

В правилах Хебба нет ни слова о том, что веса связей не могут принимать отрицательные значения или увеличиваться до бесконечности. Если возникла необходимость ограничения, следует дописать пару строчек кода:

```javascript
if (strengths > 1) {strengths = 1};
if (strengths < 0) {strengths = 0};
```
В моей модели такое ограничение не нужно, поскольку начальные данные чётко ограничены значениями от 0 до 1.

Не забываем о необходимости вызывать процедуру обновления весов определённое множество раз *iterations*, так чтобы весовые коэффициенты были в итоге приведены к своим конечным значениям.
Обычно хеббовские нейронные сети обучаются очень медленно. Мы можем убедиться в том, что часто входные образы приходится предъявлять тысячами раз и несмотря на это, сеть может обучаться неверно. Если имеется, к примеру, 8 обучающих образов - то придется предъявить один образ, модифицировать веса и повторить процедуру для всех остальных образов, а затем повторять и повторять этот цикл столько раз, сколько потребуется для получения правильных результатов.

